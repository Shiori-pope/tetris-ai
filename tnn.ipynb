{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be41153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tetris import *\n",
    "from display import *\n",
    "from tnn import TetrisNN,TetrisNN3,TNN_E\n",
    "from dataset_generator.datapaster import txt_to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bc623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理文件种子: 10272\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 10542\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 1131\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 11902\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 11920\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 11987\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 12481\n",
      "  总行数: 3911, 保留: 3871, 忽略: 40\n",
      "  已处理 3871 条操作记录\n",
      "处理文件种子: 13388\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 1350\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 1360\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 13763\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 14121\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 14274\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 14543\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 15596\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 16676\n",
      "  总行数: 8609, 保留: 8522, 忽略: 87\n",
      "  已处理 8522 条操作记录\n",
      "处理文件种子: 17090\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 1720\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 17536\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 17577\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 18342\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 18762\n",
      "  总行数: 9610, 保留: 9513, 忽略: 97\n",
      "  已处理 9513 条操作记录\n",
      "处理文件种子: 18941\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 20580\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 20716\n",
      "  总行数: 2931, 保留: 2901, 忽略: 30\n",
      "  已处理 2901 条操作记录\n",
      "处理文件种子: 2087\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 21839\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 21846\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 21891\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 2208\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 2215\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 2374\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 23821\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 24042\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 2439\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 24479\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 2570\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 26912\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 27974\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 29202\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 29340\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 29856\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 30073\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 30408\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 30702\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 31089\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 3174\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 34111\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 34418\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 34673\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 3496\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 35492\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 3632\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 37277\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 39480\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 40361\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 42476\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 43095\n",
      "  总行数: 1797, 保留: 1779, 忽略: 18\n",
      "  已处理 1779 条操作记录\n",
      "处理文件种子: 43834\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 44340\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 44628\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 4613\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 4717\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 47737\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 4804\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 4823\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 49288\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 50018\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 50279\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 50571\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 51628\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 52509\n",
      "  总行数: 5278, 保留: 5225, 忽略: 53\n",
      "  已处理 5225 条操作记录\n",
      "处理文件种子: 52927\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 53226\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 5347\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 55084\n",
      "  总行数: 4089, 保留: 4048, 忽略: 41\n",
      "  已处理 4048 条操作记录\n",
      "处理文件种子: 55251\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 5558\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 55893\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 56185\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 56376\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 5744\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 58603\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 59131\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 59371\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 60211\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 60610\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 6305\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 63526\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 6416\n",
      "  总行数: 9845, 保留: 9746, 忽略: 99\n",
      "  已处理 9746 条操作记录\n",
      "处理文件种子: 64454\n",
      "  总行数: 7296, 保留: 7223, 忽略: 73\n",
      "  已处理 7223 条操作记录\n",
      "处理文件种子: 65101\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 66147\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 66654\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 676\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 6830\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 68654\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 69495\n",
      "  总行数: 1375, 保留: 1361, 忽略: 14\n",
      "  已处理 1361 条操作记录\n",
      "处理文件种子: 70231\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 7025\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 71143\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 7115\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 72935\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 74607\n",
      "  总行数: 3700, 保留: 3663, 忽略: 37\n",
      "  已处理 3663 条操作记录\n",
      "处理文件种子: 75726\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 79277\n",
      "  总行数: 7464, 保留: 7389, 忽略: 75\n",
      "  已处理 7389 条操作记录\n",
      "处理文件种子: 79790\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 80046\n",
      "  总行数: 1675, 保留: 1658, 忽略: 17\n",
      "  已处理 1658 条操作记录\n",
      "处理文件种子: 8144\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 81889\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 83827\n",
      "  总行数: 1011, 保留: 1000, 忽略: 11\n",
      "  已处理 1000 条操作记录\n",
      "处理文件种子: 8397\n",
      "  总行数: 4436, 保留: 4391, 忽略: 45\n",
      "  已处理 4391 条操作记录\n",
      "处理文件种子: 8467\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 85166\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 87460\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 87594\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 88746\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 88770\n",
      "  总行数: 9723, 保留: 9625, 忽略: 98\n",
      "  已处理 9625 条操作记录\n",
      "处理文件种子: 90033\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 90361\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 90826\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 90946\n",
      "  总行数: 8756, 保留: 8668, 忽略: 88\n",
      "  已处理 8668 条操作记录\n",
      "处理文件种子: 92059\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 94307\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 96458\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 97938\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 98174\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 99204\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n",
      "处理文件种子: 99778\n",
      "  总行数: 10000, 保留: 9900, 忽略: 100\n",
      "  已处理 9900 条操作记录\n"
     ]
    }
   ],
   "source": [
    "X,Y=txt_to_numpy('dataset_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cdd16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,X2,X3 = [], [], []\n",
    "for X1t,X2t,X3t in X:\n",
    "    X1.append(place_piece_in_center(X1t,(20,20)))\n",
    "    X2.append(place_piece_in_center(X2t,(4,4)))\n",
    "    X3.append(place_piece_in_center(X3t,(4,4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a0f1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='line-height:1.5em;font-size:12px'>⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛🟩⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛🟩⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛🟩⬛🟩🟩🟩⬛⬛⬛🟩🟩⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛🟩⬛🟩🟩🟩🟩🟩🟩🟩🟩⬛⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛⬛🟩🟩🟩🟩🟩🟩🟩🟩🟩⬛⬛⬛⬛⬛⬛</pre>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style='line-height:1.5em;font-size:12px'>⬛⬛⬛⬛\n",
       "⬛🟩🟩⬛\n",
       "⬛🟩🟩⬛\n",
       "⬛⬛⬛⬛</pre>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style='line-height:1.5em;font-size:12px'>⬛⬛⬛⬛\n",
       "🟩🟩🟩🟩\n",
       "⬛⬛⬛⬛\n",
       "⬛⬛⬛⬛</pre>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    display_html(board_to_html(X1[i]))\n",
    "    display_html(board_to_html(X2[i]))\n",
    "    display_html(board_to_html(X3[i]))\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d365948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199383, 1, 20, 20) (1199383, 4, 4, 4) (1199383, 4, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "X1,X2,X3 = np.array(X1,dtype=np.float64),np.array(X2,dtype=np.float64),np.array(X3,dtype=np.float64)\n",
    "X1 = X1[:,np.newaxis]\n",
    "X2 = expand_to_4_rotations(X2)\n",
    "X3 = expand_to_4_rotations(X3)\n",
    "print(X1.shape,X2.shape,X3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb5f464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1199383, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y= np.array(Y, dtype=np.float64)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e93120ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成精英数据集\n",
    "# # 前一个X1的sum+4 - 10n(n取1 2 3 4) =后一个X1\n",
    "# # X1 shape(batch, 20, 10)\n",
    "# X1_front = X1[:-1]\n",
    "# X1_back = X1[1:]\n",
    "# print(X1_front.shape)\n",
    "# X1_front_sum = np.sum(X1_front, axis=(1,2, 3)) + 4\n",
    "# X1_back_sum = np.sum(X1_back, axis=(1,2, 3))\n",
    "# line1 = (X1_front_sum - 10) == X1_back_sum\n",
    "# line2 = (X1_front_sum - 20) == X1_back_sum\n",
    "# line3 = (X1_front_sum - 30) == X1_back_sum\n",
    "# line4 = (X1_front_sum - 40) == X1_back_sum\n",
    "# print(f\"line1: {np.sum(line1)}, line2: {np.sum(line2)}, line3: {np.sum(line3)}, line4: {np.sum(line4)}\")\n",
    "# X1_super = X1[:-1][line1 | line2 | line3 | line4]\n",
    "# X2_super = X2[:-1][line1 | line2 | line3 | line4]\n",
    "# X3_super = X3[:-1][line1 | line2 | line3 | line4]\n",
    "# Y_super = Y[:-1][line1 | line2 | line3 | line4]\n",
    "# print(f\"精英数据集大小: {X1_super.shape}\")\n",
    "# print(X1_front_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0449b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def expand_to_4_rotations(X):\n",
    "    # X: (N, 4, 4) -> (N, 4, 4, 4)\n",
    "    rotations = [np.rot90(X, k=i, axes=(1, 2)) for i in range(4)]  # 每次绕 (4,4) 平面旋转\n",
    "    return np.stack(rotations, axis=1)  # (N, 4, 4, 4)\n",
    "\n",
    "# 1. 数据预处理函数（保持不变）\n",
    "def prepare_dual_stream_data(X1, X2, X3, Y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    准备双流网络的数据\n",
    "    X1: 棋盘状态 (N, 20, 10)\n",
    "    X2: 当前方块 (N, 4, 4) \n",
    "    X3: 下一个方块 (N, 4, 4)\n",
    "    Y: 标签 (N, 3)\n",
    "    \"\"\"\n",
    "    print(f\"棋盘数据形状: {X1.shape}\")\n",
    "    print(f\"当前方块数据形状: {X2.shape}\")\n",
    "    print(f\"下一个方块数据形状: {X3.shape}\")\n",
    "    print(f\"标签数据形状: {Y.shape}\")\n",
    "    \n",
    "    # 打乱数据\n",
    "    indices = np.random.permutation(len(X1))\n",
    "    X1_shuffled = X1[indices]\n",
    "    X2_shuffled = X2[indices]\n",
    "    X3_shuffled = X3[indices]\n",
    "    Y_shuffled = Y[indices]\n",
    "    \n",
    "    # 分割训练集和测试集\n",
    "    X1_train, X1_test, X2_train,X2_test, X3_train,X3_test, Y_train, Y_test = train_test_split(\n",
    "        X1_shuffled, X2_shuffled, X3_shuffled, Y_shuffled, \n",
    "        test_size=test_size, random_state=random_state,\n",
    "        stratify=Y_shuffled[:, 1]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n训练集:\")\n",
    "    print(f\"  棋盘: {X1_train.shape}\")\n",
    "    print(f\"  当前方块: {X2_train.shape}\")\n",
    "    print(f\"  下一个方块: {X3_train.shape}\")\n",
    "    print(f\"  标签: {Y_train.shape}\")\n",
    "    print(f\"测试集:\")\n",
    "    print(f\"  棋盘: {X1_test.shape}\")\n",
    "    print(f\"  当前方块: {X2_test.shape}\")\n",
    "    print(f\"  下一个方块: {X3_test.shape}\")\n",
    "    print(f\"  标签: {Y_test.shape}\")\n",
    "    \n",
    "    return X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, Y_train, Y_test\n",
    "# X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, Y_train, Y_test = prepare_dual_stream_data(X1, X2, X3, Y,test_size=0.01)\n",
    "# 精英数据集构造\n",
    "# X1_super_train, X1_super_test, X2_super_train, X2_super_test, X3_super_train, X3_super_test, Y_super_train, Y_super_test = prepare_dual_stream_data(X1_super, X2_super, X3_super, Y_super,test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3356db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 数据集\n",
    "class TetrisDataset(Dataset):\n",
    "    def __init__(self, X1, X2, X3, Y):\n",
    "        self.X1 = torch.FloatTensor(X1)\n",
    "        self.X2 = torch.FloatTensor(X2)\n",
    "        self.X3 = torch.FloatTensor(X3)\n",
    "        self.Y = torch.LongTensor(Y)\n",
    "    def __len__(self): return len(self.X1)\n",
    "    def __getitem__(self, idx): return self.X1[idx], self.X2[idx], self.X3[idx], self.Y[idx]\n",
    "\n",
    "# 标签编码\n",
    "def encode_labels(Y):\n",
    "    Y = Y.copy()\n",
    "    Y[:,0] += 4\n",
    "    Y[:,2] += 1\n",
    "    return Y.astype(np.int64)\n",
    "\n",
    "# 划分数据\n",
    "def split_data(X1, X2, X3, Y, test_size=0.2):\n",
    "    idx = np.random.permutation(len(X1))\n",
    "    split = int(len(X1)*(1-test_size))\n",
    "    return (X1[idx[:split]], X2[idx[:split]], X3[idx[:split]], Y[idx[:split]],\n",
    "            X1[idx[split:]], X2[idx[split:]], X3[idx[split:]], Y[idx[split:]])\n",
    "\n",
    "# 训练主流程\n",
    "def train_cnn(model,X1, X2, X3, Y, epochs=20, batch_size=256, lr=1e-3):\n",
    "    Y = encode_labels(Y)\n",
    "    X1_tr, X2_tr, X3_tr, Y_tr, X1_te, X2_te, X3_te, Y_te = split_data(X1, X2, X3, Y)\n",
    "    train_loader = DataLoader(TetrisDataset(X1_tr, X2_tr, X3_tr, Y_tr), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TetrisDataset(X1_te, X2_te, X3_te, Y_te), batch_size=batch_size)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accs, test_accs = [], []\n",
    "    train_acc_col, train_acc_rot, train_acc_slide = [], [], []\n",
    "    test_acc_col, test_acc_rot, test_acc_slide = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct_col, correct_rot, correct_slide, total = 0,0,0,0,0\n",
    "\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        for xb, xn, xx, y in train_loader:\n",
    "            xb, xn, xx, y = xb.to(device), xn.to(device), xx.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out_col, out_rot, out_slide = model(xb, xn, xx)\n",
    "                loss = 5*loss_fn(out_col, y[:,0]) + 3*loss_fn(out_rot, y[:,1]) + 0.5*loss_fn(out_slide, y[:,2])\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            correct_col += (out_col.argmax(1) == y[:,0]).sum().item()\n",
    "            correct_rot += (out_rot.argmax(1) == y[:,1]).sum().item()\n",
    "            correct_slide += (out_slide.argmax(1) == y[:,2]).sum().item()\n",
    "            total += xb.size(0)\n",
    "            \n",
    "        train_losses.append(total_loss/total)\n",
    "        train_acc_col.append(correct_col/total)\n",
    "        train_acc_rot.append(correct_rot/total)\n",
    "        train_acc_slide.append(correct_slide/total)\n",
    "        train_accs.append((correct_col+correct_rot+correct_slide)/(3*total))\n",
    "\n",
    "        # 测试\n",
    "        model.eval()\n",
    "        total_loss, correct_col, correct_rot, correct_slide, total = 0,0,0,0,0\n",
    "        with torch.no_grad():\n",
    "            for xb, xn, xx, y in test_loader:\n",
    "                xb, xn, xx, y = xb.to(device), xn.to(device), xx.to(device), y.to(device)\n",
    "                out_col, out_rot, out_slide = model(xb, xn, xx)\n",
    "                loss = 5*loss_fn(out_col, y[:,0]) + 3*loss_fn(out_rot, y[:,1]) + 0.3*loss_fn(out_slide, y[:,2])\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "                correct_col += (out_col.argmax(1) == y[:,0]).sum().item()\n",
    "                correct_rot += (out_rot.argmax(1) == y[:,1]).sum().item()\n",
    "                correct_slide += (out_slide.argmax(1) == y[:,2]).sum().item()\n",
    "                total += xb.size(0)\n",
    "        test_losses.append(total_loss/total)\n",
    "        test_acc_col.append(correct_col/total)\n",
    "        test_acc_rot.append(correct_rot/total)\n",
    "        test_acc_slide.append(correct_slide/total)\n",
    "        test_accs.append((correct_col+correct_rot+correct_slide)/(3*total))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Train Loss: {train_losses[-1]:.4f} Test Loss: {test_losses[-1]:.4f} \"\n",
    "              f\"Train Acc: {train_accs[-1]*100:.2f}% Test Acc: {test_accs[-1]*100:.2f}%\")\n",
    "        print(f\"colacc: {train_acc_col[-1]*100:.2f}% rotacc: {train_acc_rot[-1]*100:.2f}% slideacc: {train_acc_slide[-1]*100:.2f}%\")\n",
    "\n",
    "    # 画图\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.legend(); plt.title('Loss')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(test_accs, label='Test Acc')\n",
    "    plt.plot(train_acc_col, label='Train Col')\n",
    "    plt.plot(test_acc_col, label='Test Col')\n",
    "    plt.plot(train_acc_rot, label='Train Rot')\n",
    "    plt.plot(test_acc_rot, label='Test Rot')\n",
    "    plt.plot(train_acc_slide, label='Train Slide')\n",
    "    plt.plot(test_acc_slide, label='Test Slide')\n",
    "    plt.legend(); plt.title('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "# 用法示例（假设X1, X2, X3, Y已准备好，且X1:(N,1,20,20), X2/X3:(N,4,4,4), Y:(N,3)）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44fe719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'tetris_model3.1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac2076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('tetris_model3.0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35579c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(modelE.state_dict(), 'tetris_model3_1.2M.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07819921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_26528\\2159293767.py:51: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Temp\\ipykernel_26528\\2159293767.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Train Loss: 14.4075 Test Loss: 12.7851 Train Acc: 55.43% Test Acc: 61.63%\n",
      "colacc: 15.54% rotacc: 53.42% slideacc: 97.33%\n",
      "Epoch 2/200 Train Loss: 11.9923 Test Loss: 11.0813 Train Acc: 64.51% Test Acc: 67.75%\n",
      "colacc: 29.22% rotacc: 64.50% slideacc: 99.82%\n",
      "Epoch 3/200 Train Loss: 10.8860 Test Loss: 10.1922 Train Acc: 68.76% Test Acc: 70.99%\n",
      "colacc: 36.93% rotacc: 69.52% slideacc: 99.82%\n",
      "Epoch 4/200 Train Loss: 10.2622 Test Loss: 9.6991 Train Acc: 71.04% Test Acc: 72.31%\n",
      "colacc: 40.88% rotacc: 72.43% slideacc: 99.82%\n",
      "Epoch 5/200 Train Loss: 9.7162 Test Loss: 8.8583 Train Acc: 72.96% Test Acc: 75.91%\n",
      "colacc: 44.69% rotacc: 74.37% slideacc: 99.82%\n",
      "Epoch 6/200 Train Loss: 9.1501 Test Loss: 8.3909 Train Acc: 74.92% Test Acc: 77.11%\n",
      "colacc: 48.86% rotacc: 76.07% slideacc: 99.82%\n",
      "Epoch 7/200 Train Loss: 8.6470 Test Loss: 7.6030 Train Acc: 76.70% Test Acc: 80.59%\n",
      "colacc: 52.52% rotacc: 77.75% slideacc: 99.82%\n",
      "Epoch 8/200 Train Loss: 8.1947 Test Loss: 7.1706 Train Acc: 78.33% Test Acc: 81.82%\n",
      "colacc: 55.72% rotacc: 79.44% slideacc: 99.82%\n",
      "Epoch 9/200 Train Loss: 7.8040 Test Loss: 6.6769 Train Acc: 79.62% Test Acc: 83.37%\n",
      "colacc: 58.43% rotacc: 80.60% slideacc: 99.82%\n",
      "Epoch 10/200 Train Loss: 7.4598 Test Loss: 6.5493 Train Acc: 80.67% Test Acc: 83.49%\n",
      "colacc: 60.70% rotacc: 81.47% slideacc: 99.82%\n",
      "Epoch 11/200 Train Loss: 7.2040 Test Loss: 6.0941 Train Acc: 81.46% Test Acc: 84.75%\n",
      "colacc: 62.42% rotacc: 82.12% slideacc: 99.82%\n",
      "Epoch 12/200 Train Loss: 6.9630 Test Loss: 5.9239 Train Acc: 82.16% Test Acc: 85.12%\n",
      "colacc: 63.97% rotacc: 82.68% slideacc: 99.82%\n",
      "Epoch 13/200 Train Loss: 6.7712 Test Loss: 5.8357 Train Acc: 82.69% Test Acc: 85.20%\n",
      "colacc: 65.17% rotacc: 83.07% slideacc: 99.82%\n",
      "Epoch 14/200 Train Loss: 6.6157 Test Loss: 5.4135 Train Acc: 83.15% Test Acc: 86.45%\n",
      "colacc: 66.19% rotacc: 83.45% slideacc: 99.82%\n",
      "Epoch 15/200 Train Loss: 6.4502 Test Loss: 5.3988 Train Acc: 83.63% Test Acc: 86.50%\n",
      "colacc: 67.22% rotacc: 83.84% slideacc: 99.82%\n",
      "Epoch 16/200 Train Loss: 6.3346 Test Loss: 5.2334 Train Acc: 83.96% Test Acc: 86.79%\n",
      "colacc: 67.90% rotacc: 84.14% slideacc: 99.82%\n",
      "Epoch 17/200 Train Loss: 6.2111 Test Loss: 5.1158 Train Acc: 84.30% Test Acc: 87.17%\n",
      "colacc: 68.67% rotacc: 84.41% slideacc: 99.82%\n",
      "Epoch 18/200 Train Loss: 6.0897 Test Loss: 5.0351 Train Acc: 84.62% Test Acc: 87.35%\n",
      "colacc: 69.33% rotacc: 84.71% slideacc: 99.82%\n",
      "Epoch 19/200 Train Loss: 6.0128 Test Loss: 4.9036 Train Acc: 84.85% Test Acc: 87.56%\n",
      "colacc: 69.88% rotacc: 84.85% slideacc: 99.82%\n",
      "Epoch 20/200 Train Loss: 5.9171 Test Loss: 4.9450 Train Acc: 85.10% Test Acc: 87.49%\n",
      "colacc: 70.39% rotacc: 85.09% slideacc: 99.82%\n",
      "Epoch 21/200 Train Loss: 5.8404 Test Loss: 4.7063 Train Acc: 85.32% Test Acc: 88.18%\n",
      "colacc: 70.87% rotacc: 85.26% slideacc: 99.82%\n",
      "Epoch 22/200 Train Loss: 5.7494 Test Loss: 4.7079 Train Acc: 85.58% Test Acc: 88.10%\n",
      "colacc: 71.40% rotacc: 85.50% slideacc: 99.82%\n",
      "Epoch 23/200 Train Loss: 5.6947 Test Loss: 4.5553 Train Acc: 85.71% Test Acc: 88.63%\n",
      "colacc: 71.72% rotacc: 85.59% slideacc: 99.82%\n",
      "Epoch 24/200 Train Loss: 5.6101 Test Loss: 4.5644 Train Acc: 85.94% Test Acc: 88.43%\n",
      "colacc: 72.18% rotacc: 85.80% slideacc: 99.82%\n",
      "Epoch 25/200 Train Loss: 5.5581 Test Loss: 4.6920 Train Acc: 86.08% Test Acc: 88.15%\n",
      "colacc: 72.47% rotacc: 85.94% slideacc: 99.82%\n",
      "Epoch 26/200 Train Loss: 5.5059 Test Loss: 4.3771 Train Acc: 86.22% Test Acc: 89.01%\n",
      "colacc: 72.82% rotacc: 86.03% slideacc: 99.82%\n",
      "Epoch 27/200 Train Loss: 5.4411 Test Loss: 4.3484 Train Acc: 86.40% Test Acc: 89.05%\n",
      "colacc: 73.17% rotacc: 86.21% slideacc: 99.82%\n",
      "Epoch 28/200 Train Loss: 5.3888 Test Loss: 4.3094 Train Acc: 86.54% Test Acc: 89.24%\n",
      "colacc: 73.46% rotacc: 86.34% slideacc: 99.82%\n",
      "Epoch 29/200 Train Loss: 5.3361 Test Loss: 4.2698 Train Acc: 86.70% Test Acc: 89.22%\n",
      "colacc: 73.80% rotacc: 86.48% slideacc: 99.82%\n",
      "Epoch 30/200 Train Loss: 5.2856 Test Loss: 4.2261 Train Acc: 86.80% Test Acc: 89.25%\n",
      "colacc: 73.99% rotacc: 86.59% slideacc: 99.82%\n",
      "Epoch 31/200 Train Loss: 5.2428 Test Loss: 4.1410 Train Acc: 86.94% Test Acc: 89.55%\n",
      "colacc: 74.30% rotacc: 86.69% slideacc: 99.82%\n",
      "Epoch 32/200 Train Loss: 5.1917 Test Loss: 4.1622 Train Acc: 87.06% Test Acc: 89.50%\n",
      "colacc: 74.59% rotacc: 86.77% slideacc: 99.82%\n",
      "Epoch 33/200 Train Loss: 5.1503 Test Loss: 4.0452 Train Acc: 87.17% Test Acc: 89.80%\n",
      "colacc: 74.79% rotacc: 86.88% slideacc: 99.82%\n",
      "Epoch 34/200 Train Loss: 5.1107 Test Loss: 4.0296 Train Acc: 87.25% Test Acc: 89.81%\n",
      "colacc: 75.02% rotacc: 86.91% slideacc: 99.82%\n",
      "Epoch 35/200 Train Loss: 5.0685 Test Loss: 4.0108 Train Acc: 87.38% Test Acc: 89.77%\n",
      "colacc: 75.26% rotacc: 87.07% slideacc: 99.82%\n",
      "Epoch 36/200 Train Loss: 5.0499 Test Loss: 3.9281 Train Acc: 87.41% Test Acc: 90.03%\n",
      "colacc: 75.33% rotacc: 87.09% slideacc: 99.82%\n",
      "Epoch 37/200 Train Loss: 4.9985 Test Loss: 3.9470 Train Acc: 87.55% Test Acc: 89.90%\n",
      "colacc: 75.65% rotacc: 87.18% slideacc: 99.82%\n",
      "Epoch 38/200 Train Loss: 4.9638 Test Loss: 4.0196 Train Acc: 87.61% Test Acc: 89.71%\n",
      "colacc: 75.75% rotacc: 87.26% slideacc: 99.82%\n",
      "Epoch 39/200 Train Loss: 4.9476 Test Loss: 4.0254 Train Acc: 87.67% Test Acc: 89.74%\n",
      "colacc: 75.92% rotacc: 87.26% slideacc: 99.82%\n",
      "Epoch 40/200 Train Loss: 4.9144 Test Loss: 3.9067 Train Acc: 87.74% Test Acc: 89.97%\n",
      "colacc: 76.08% rotacc: 87.32% slideacc: 99.82%\n",
      "Epoch 41/200 Train Loss: 4.8955 Test Loss: 3.7938 Train Acc: 87.80% Test Acc: 90.36%\n",
      "colacc: 76.17% rotacc: 87.39% slideacc: 99.82%\n",
      "Epoch 42/200 Train Loss: 4.8498 Test Loss: 3.7852 Train Acc: 87.93% Test Acc: 90.34%\n",
      "colacc: 76.46% rotacc: 87.49% slideacc: 99.82%\n",
      "Epoch 43/200 Train Loss: 4.8236 Test Loss: 3.7738 Train Acc: 87.97% Test Acc: 90.32%\n",
      "colacc: 76.57% rotacc: 87.51% slideacc: 99.82%\n",
      "Epoch 44/200 Train Loss: 4.8048 Test Loss: 3.7592 Train Acc: 88.03% Test Acc: 90.39%\n",
      "colacc: 76.72% rotacc: 87.55% slideacc: 99.82%\n",
      "Epoch 45/200 Train Loss: 4.7827 Test Loss: 3.7656 Train Acc: 88.07% Test Acc: 90.40%\n",
      "colacc: 76.81% rotacc: 87.57% slideacc: 99.82%\n",
      "Epoch 46/200 Train Loss: 4.7437 Test Loss: 3.7133 Train Acc: 88.14% Test Acc: 90.46%\n",
      "colacc: 76.95% rotacc: 87.66% slideacc: 99.82%\n",
      "Epoch 47/200 Train Loss: 4.7351 Test Loss: 3.6818 Train Acc: 88.17% Test Acc: 90.57%\n",
      "colacc: 76.99% rotacc: 87.69% slideacc: 99.82%\n",
      "Epoch 48/200 Train Loss: 4.7032 Test Loss: 3.7004 Train Acc: 88.27% Test Acc: 90.47%\n",
      "colacc: 77.19% rotacc: 87.78% slideacc: 99.82%\n",
      "Epoch 49/200 Train Loss: 4.6836 Test Loss: 3.6781 Train Acc: 88.29% Test Acc: 90.56%\n",
      "colacc: 77.25% rotacc: 87.80% slideacc: 99.82%\n",
      "Epoch 50/200 Train Loss: 4.6680 Test Loss: 3.7613 Train Acc: 88.34% Test Acc: 90.34%\n",
      "colacc: 77.37% rotacc: 87.82% slideacc: 99.82%\n",
      "Epoch 51/200 Train Loss: 4.6544 Test Loss: 3.6737 Train Acc: 88.35% Test Acc: 90.47%\n",
      "colacc: 77.42% rotacc: 87.82% slideacc: 99.82%\n",
      "Epoch 52/200 Train Loss: 4.6425 Test Loss: 3.6117 Train Acc: 88.40% Test Acc: 90.65%\n",
      "colacc: 77.50% rotacc: 87.86% slideacc: 99.82%\n",
      "Epoch 53/200 Train Loss: 4.6140 Test Loss: 3.6055 Train Acc: 88.46% Test Acc: 90.71%\n",
      "colacc: 77.62% rotacc: 87.93% slideacc: 99.82%\n",
      "Epoch 54/200 Train Loss: 4.6003 Test Loss: 3.7126 Train Acc: 88.50% Test Acc: 90.42%\n",
      "colacc: 77.73% rotacc: 87.95% slideacc: 99.82%\n",
      "Epoch 55/200 Train Loss: 4.5890 Test Loss: 3.5850 Train Acc: 88.53% Test Acc: 90.70%\n",
      "colacc: 77.79% rotacc: 87.97% slideacc: 99.82%\n",
      "Epoch 56/200 Train Loss: 4.5560 Test Loss: 3.5885 Train Acc: 88.59% Test Acc: 90.72%\n",
      "colacc: 77.92% rotacc: 88.04% slideacc: 99.82%\n",
      "Epoch 57/200 Train Loss: 4.5632 Test Loss: 3.8210 Train Acc: 88.56% Test Acc: 90.11%\n",
      "colacc: 77.86% rotacc: 88.00% slideacc: 99.82%\n",
      "Epoch 58/200 Train Loss: 4.5552 Test Loss: 3.5168 Train Acc: 88.57% Test Acc: 90.92%\n",
      "colacc: 77.89% rotacc: 87.99% slideacc: 99.82%\n",
      "Epoch 59/200 Train Loss: 4.5157 Test Loss: 3.5789 Train Acc: 88.70% Test Acc: 90.76%\n",
      "colacc: 78.17% rotacc: 88.11% slideacc: 99.82%\n",
      "Epoch 60/200 Train Loss: 4.5049 Test Loss: 3.4993 Train Acc: 88.71% Test Acc: 90.90%\n",
      "colacc: 78.20% rotacc: 88.10% slideacc: 99.82%\n",
      "Epoch 61/200 Train Loss: 4.4850 Test Loss: 3.5152 Train Acc: 88.76% Test Acc: 90.87%\n",
      "colacc: 78.31% rotacc: 88.15% slideacc: 99.82%\n",
      "Epoch 62/200 Train Loss: 4.4801 Test Loss: 3.5479 Train Acc: 88.79% Test Acc: 90.75%\n",
      "colacc: 78.33% rotacc: 88.22% slideacc: 99.82%\n",
      "Epoch 63/200 Train Loss: 4.4769 Test Loss: 3.4632 Train Acc: 88.78% Test Acc: 90.98%\n",
      "colacc: 78.32% rotacc: 88.20% slideacc: 99.82%\n",
      "Epoch 64/200 Train Loss: 4.4563 Test Loss: 3.4798 Train Acc: 88.82% Test Acc: 90.94%\n",
      "colacc: 78.41% rotacc: 88.24% slideacc: 99.82%\n",
      "Epoch 65/200 Train Loss: 4.4334 Test Loss: 3.5004 Train Acc: 88.88% Test Acc: 90.93%\n",
      "colacc: 78.57% rotacc: 88.26% slideacc: 99.82%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m TetrisNN(dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m train_cnn(model,X1, X2, X3, Y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7000\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7e-4\u001b[39m )\n",
      "Cell \u001b[1;32mIn[7], line 59\u001b[0m, in \u001b[0;36mtrain_cnn\u001b[1;34m(model, X1, X2, X3, Y, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     57\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn(out_col, y[:,\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn(out_rot, y[:,\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mloss_fn(out_slide, y[:,\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     58\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 59\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(opt)\n\u001b[0;32m     60\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     61\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:461\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    459\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 461\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    463\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    349\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    354\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    356\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    349\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    354\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    356\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TetrisNN(dropout_rate=0.3)\n",
    "train_cnn(model,X1, X2, X3, Y, epochs=200, batch_size=7000, lr=7e-4 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/200 Train Loss: 8.0070 Test Loss: 5.1802 Train Acc: 78.02% Test Acc: 86.50%\n",
    "# colacc: 56.27% rotacc: 80.95% slideacc: 96.86%\n",
    "# Epoch 2/200 Train Loss: 4.7027 Test Loss: 3.7801 Train Acc: 88.02% Test Acc: 90.25%\n",
    "# colacc: 76.29% rotacc: 87.96% slideacc: 99.82%\n",
    "# Epoch 3/200 Train Loss: 3.9397 Test Loss: 3.3731 Train Acc: 89.76% Test Acc: 90.96%\n",
    "# colacc: 80.13% rotacc: 89.33% slideacc: 99.82%\n",
    "# Epoch 4/200 Train Loss: 3.5542 Test Loss: 3.0237 Train Acc: 90.60% Test Acc: 91.68%\n",
    "# colacc: 81.92% rotacc: 90.04% slideacc: 99.82%\n",
    "# Epoch 5/200 Train Loss: 3.2958 Test Loss: 2.9695 Train Acc: 91.15% Test Acc: 91.63%\n",
    "# colacc: 83.07% rotacc: 90.56% slideacc: 99.82%\n",
    "# Epoch 6/200 Train Loss: 3.1107 Test Loss: 2.7794 Train Acc: 91.55% Test Acc: 92.22%\n",
    "# colacc: 83.91% rotacc: 90.91% slideacc: 99.82%\n",
    "# Epoch 7/200 Train Loss: 2.9600 Test Loss: 2.6682 Train Acc: 91.91% Test Acc: 92.53%\n",
    "# colacc: 84.66% rotacc: 91.24% slideacc: 99.83%\n",
    "# Epoch 8/200 Train Loss: 2.8462 Test Loss: 2.5963 Train Acc: 92.14% Test Acc: 92.65%\n",
    "# colacc: 85.09% rotacc: 91.50% slideacc: 99.83%\n",
    "# Epoch 9/200 Train Loss: 2.7459 Test Loss: 2.5245 Train Acc: 92.40% Test Acc: 92.82%\n",
    "# colacc: 85.61% rotacc: 91.74% slideacc: 99.84%\n",
    "# Epoch 10/200 Train Loss: 2.6553 Test Loss: 2.4124 Train Acc: 92.61% Test Acc: 93.08%\n",
    "# colacc: 86.02% rotacc: 91.95% slideacc: 99.84%\n",
    "# Epoch 11/200 Train Loss: 2.5758 Test Loss: 2.4321 Train Acc: 92.81% Test Acc: 93.03%\n",
    "# colacc: 86.43% rotacc: 92.16% slideacc: 99.85%\n",
    "# Epoch 12/200 Train Loss: 2.5066 Test Loss: 2.4015 Train Acc: 92.97% Test Acc: 93.16%\n",
    "# colacc: 86.76% rotacc: 92.29% slideacc: 99.86%\n",
    "# Epoch 13/200 Train Loss: 2.4344 Test Loss: 2.3365 Train Acc: 93.13% Test Acc: 93.32%\n",
    "# colacc: 87.06% rotacc: 92.47% slideacc: 99.86%\n",
    "# Epoch 14/200 Train Loss: 2.3720 Test Loss: 2.4456 Train Acc: 93.30% Test Acc: 93.04%\n",
    "# colacc: 87.40% rotacc: 92.64% slideacc: 99.87%\n",
    "# Epoch 15/200 Train Loss: 2.3155 Test Loss: 2.4061 Train Acc: 93.45% Test Acc: 93.07%\n",
    "# colacc: 87.70% rotacc: 92.76% slideacc: 99.88%\n",
    "# Epoch 16/200 Train Loss: 2.2625 Test Loss: 2.3038 Train Acc: 93.61% Test Acc: 93.55%\n",
    "# colacc: 88.02% rotacc: 92.93% slideacc: 99.88%\n",
    "# Epoch 17/200 Train Loss: 2.2054 Test Loss: 2.3438 Train Acc: 93.73% Test Acc: 93.39%\n",
    "# colacc: 88.25% rotacc: 93.06% slideacc: 99.88%\n",
    "# Epoch 18/200 Train Loss: 2.1554 Test Loss: 2.3064 Train Acc: 93.87% Test Acc: 93.62%\n",
    "# colacc: 88.51% rotacc: 93.21% slideacc: 99.89%\n",
    "# Epoch 19/200 Train Loss: 2.1127 Test Loss: 2.2780 Train Acc: 93.97% Test Acc: 93.54%\n",
    "# colacc: 88.69% rotacc: 93.33% slideacc: 99.89%\n",
    "# Epoch 20/200 Train Loss: 2.0645 Test Loss: 2.2616 Train Acc: 94.10% Test Acc: 93.61%\n",
    "# colacc: 88.96% rotacc: 93.47% slideacc: 99.89%\n",
    "# Epoch 21/200 Train Loss: 2.0255 Test Loss: 2.3875 Train Acc: 94.21% Test Acc: 93.64%\n",
    "# colacc: 89.16% rotacc: 93.57% slideacc: 99.90%\n",
    "# Epoch 22/200 Train Loss: 1.9770 Test Loss: 2.1892 Train Acc: 94.32% Test Acc: 93.83%\n",
    "# colacc: 89.39% rotacc: 93.68% slideacc: 99.90%\n",
    "# Epoch 23/200 Train Loss: 1.9422 Test Loss: 2.3474 Train Acc: 94.43% Test Acc: 93.59%\n",
    "# colacc: 89.59% rotacc: 93.79% slideacc: 99.90%\n",
    "# Epoch 24/200 Train Loss: 1.9044 Test Loss: 2.2467 Train Acc: 94.53% Test Acc: 93.77%\n",
    "# colacc: 89.81% rotacc: 93.89% slideacc: 99.90%\n",
    "# Epoch 25/200 Train Loss: 1.8690 Test Loss: 2.1408 Train Acc: 94.61% Test Acc: 94.00%\n",
    "# colacc: 89.95% rotacc: 93.99% slideacc: 99.90%\n",
    "# Epoch 26/200 Train Loss: 1.8338 Test Loss: 2.2700 Train Acc: 94.71% Test Acc: 93.89%\n",
    "# colacc: 90.16% rotacc: 94.08% slideacc: 99.91%\n",
    "# Epoch 27/200 Train Loss: 1.7969 Test Loss: 2.2270 Train Acc: 94.82% Test Acc: 93.99%\n",
    "# colacc: 90.35% rotacc: 94.19% slideacc: 99.91%\n",
    "# Epoch 28/200 Train Loss: 1.7579 Test Loss: 2.2123 Train Acc: 94.92% Test Acc: 93.95%\n",
    "# colacc: 90.55% rotacc: 94.30% slideacc: 99.91%\n",
    "# Epoch 29/200 Train Loss: 1.7394 Test Loss: 2.1793 Train Acc: 94.98% Test Acc: 93.94%\n",
    "# colacc: 90.67% rotacc: 94.37% slideacc: 99.91%\n",
    "# Epoch 30/200 Train Loss: 1.7040 Test Loss: 2.2550 Train Acc: 95.08% Test Acc: 94.01%\n",
    "# colacc: 90.84% rotacc: 94.48% slideacc: 99.92%\n",
    "# Epoch 31/200 Train Loss: 1.6726 Test Loss: 2.2619 Train Acc: 95.18% Test Acc: 94.02%\n",
    "# colacc: 91.01% rotacc: 94.62% slideacc: 99.91%\n",
    "# Epoch 32/200 Train Loss: 1.6481 Test Loss: 2.2772 Train Acc: 95.25% Test Acc: 93.93%\n",
    "# colacc: 91.18% rotacc: 94.67% slideacc: 99.92%\n",
    "# Epoch 33/200 Train Loss: 1.6172 Test Loss: 2.2249 Train Acc: 95.32% Test Acc: 94.02%\n",
    "# colacc: 91.29% rotacc: 94.76% slideacc: 99.92%\n",
    "# Epoch 34/200 Train Loss: 1.5893 Test Loss: 2.2850 Train Acc: 95.41% Test Acc: 94.13%\n",
    "# colacc: 91.49% rotacc: 94.83% slideacc: 99.92%\n",
    "# Epoch 35/200 Train Loss: 1.5664 Test Loss: 2.1629 Train Acc: 95.46% Test Acc: 94.15%\n",
    "# colacc: 91.56% rotacc: 94.90% slideacc: 99.92%\n",
    "# Epoch 36/200 Train Loss: 1.5443 Test Loss: 2.2674 Train Acc: 95.54% Test Acc: 93.99%\n",
    "# colacc: 91.72% rotacc: 94.98% slideacc: 99.92%\n",
    "# Epoch 37/200 Train Loss: 1.5170 Test Loss: 2.3618 Train Acc: 95.61% Test Acc: 93.72%\n",
    "# colacc: 91.85% rotacc: 95.07% slideacc: 99.92%\n",
    "# Epoch 38/200 Train Loss: 1.4913 Test Loss: 2.4461 Train Acc: 95.67% Test Acc: 94.16%\n",
    "# colacc: 91.98% rotacc: 95.11% slideacc: 99.92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_cnn(model,X1_train, X2_train, X3_train, Y_train, epochs=200, batch_size=8196, lr=4e-3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
